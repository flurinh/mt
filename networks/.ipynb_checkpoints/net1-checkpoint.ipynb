{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "utility-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence networks: transformer, lstm, cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "broke-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spiritual-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self, filters, kernel_size, layers, embedding, vocab_size, seq_len):\n",
    "        super(cnn, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        layers = layers\n",
    "        self.embedding = embedding\n",
    "        self.bed = nn.Embedding(self.vocab_size, self.embedding)\n",
    "        self.block1 = self.__block__(self.embedding, self.filters, ks=self.kernel_size,\n",
    "                                     pad=int((self.kernel_size - 1) / 2), drop=0.1)\n",
    "        self.block2 = self.__block__(self.filters, self.filters * 2, ks=self.kernel_size,\n",
    "                                     pad=int((self.kernel_size - 1) / 2), drop=0.1)\n",
    "        self.block3 = self.__block__(self.filters * 2, self.filters * 3, ks=self.kernel_size,\n",
    "                                     pad=int((self.kernel_size - 1) / 2), drop=0.0)\n",
    "\n",
    "        # Pooling makes our detection of features sequence position invariant\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        dense = [self.__ff_block__() for i in range(layers - 1)]\n",
    "        self.dense = nn.Sequential(*dense, self.__ff_block__(True))\n",
    "\n",
    "    def __block__(self, f_in, f_out, ks, pad, drop):\n",
    "        return nn.Sequential(\n",
    "            # nn.BatchNorm1d(f_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(f_in, f_out, kernel_size=ks, padding=pad)\n",
    "        )\n",
    "\n",
    "    def __ff_block__(self, final=False):\n",
    "        if final:\n",
    "            return nn.Sequential(\n",
    "                # nn.BatchNorm1d(self.embedding),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.filters * 3, self.seq_len)\n",
    "            )\n",
    "        return nn.Sequential(\n",
    "            # nn.BatchNorm1d(self.embedding),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.filters * 3, self.filters * 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.bed(x)\n",
    "        # print(\"embedded\", x.shape)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # print(\"permutation\", x.shape)\n",
    "        x = self.block1(x)\n",
    "        # print(\"conv1\", x.shape)\n",
    "        x = self.block2(x)\n",
    "        # print(\"conv2\", x.shape)\n",
    "        x = self.block3(x)\n",
    "        # print(\"conv3\", x.shape)\n",
    "        x = self.pool(x)\n",
    "        # print(\"pool\", x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dense(x)\n",
    "        # print(\"out\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extraordinary-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self, hidden, rnn_layers, vocab_size):\n",
    "        super(lstm, self).__init__()\n",
    "        self.specs = specs\n",
    "\n",
    "        # Dimensions\n",
    "        self._input_dim = vocab_size\n",
    "        self._hidden_dim = hidden\n",
    "        self._output_dim = vocab_size\n",
    "\n",
    "        # Number of LSTM layers\n",
    "        self._layers = rnn_layers\n",
    "\n",
    "        # LSTM mod\n",
    "        self._lstm = nn.LSTM(input_size=self._input_dim, hidden_size=self._hidden_dim, num_layers=self._layers,\n",
    "                             dropout=0.3)\n",
    "\n",
    "        # All weights initialized with xavier uniform\n",
    "        nn.init.xavier_uniform_(self._lstm.weight_ih_l0)\n",
    "        nn.init.xavier_uniform_(self._lstm.weight_ih_l1)\n",
    "        nn.init.orthogonal_(self._lstm.weight_hh_l0)\n",
    "        nn.init.orthogonal_(self._lstm.weight_hh_l1)\n",
    "\n",
    "        # Bias initialized with zeros expect the bias of the forget gate\n",
    "        self._lstm.bias_ih_l0.data.fill_(0.0)\n",
    "        self._lstm.bias_ih_l0.data[self._hidden_dim:2 * self._hidden_dim].fill_(1.0)\n",
    "\n",
    "        self._lstm.bias_ih_l1.data.fill_(0.0)\n",
    "        self._lstm.bias_ih_l1.data[self._hidden_dim:2 * self._hidden_dim].fill_(1.0)\n",
    "\n",
    "        self._lstm.bias_hh_l0.data.fill_(0.0)\n",
    "        self._lstm.bias_hh_l0.data[self._hidden_dim:2 * self._hidden_dim].fill_(1.0)\n",
    "\n",
    "        self._lstm.bias_hh_l1.data.fill_(0.0)\n",
    "        self._lstm.bias_hh_l1.data[self._hidden_dim:2 * self._hidden_dim].fill_(1.0)\n",
    "\n",
    "        # Batch normalization (Weights initialized with one and bias with zero)\n",
    "        self._norm_0 = nn.LayerNorm(self._input_dim, eps=.001)\n",
    "        self._norm_1 = nn.LayerNorm(self._hidden_dim, eps=.001)\n",
    "\n",
    "        # Separate linear model for forward and backward computation\n",
    "        self._wforward = nn.Linear(self._hidden_dim, self._output_dim)\n",
    "        nn.init.xavier_uniform_(self._wforward.weight)\n",
    "        self._wforward.bias.data.fill_(0.0)\n",
    "\n",
    "    def _init_hidden(self, batch_size, device):\n",
    "        return (torch.zeros(self._layers, batch_size, self._hidden_dim).to(device),\n",
    "                torch.zeros(self._layers, batch_size, self._hidden_dim).to(device))\n",
    "\n",
    "    def new_sequence(self, batch_size=1, device=\"cpu\"):\n",
    "        return self._init_hidden(batch_size, device)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Normalization over encoding dimension\n",
    "        norm_0 = self._norm_0(input)\n",
    "\n",
    "        # Compute LSTM unit\n",
    "        out, hidden = self._lstm(norm_0, hidden)\n",
    "\n",
    "        # Normalization over hidden dimension\n",
    "        norm_1 = self._norm_1(out)\n",
    "\n",
    "        # Linear unit forward prediction\n",
    "        forward = self._wforward(norm_1)\n",
    "\n",
    "        return forward, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-samuel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
