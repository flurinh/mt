{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defensive-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from utils2 import * \n",
    "from utils3 import *\n",
    "from plotting import *\n",
    "from gpcrdb_soup import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "atlantic-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "following-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas\n",
    "import gemmi\n",
    "from gemmi import cif\n",
    "import random\n",
    "from math import degrees\n",
    "import mplcursors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adequate-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifProcessor():\n",
    "    def __init__(self, \n",
    "                 path = 'data/',\n",
    "                 structure = 'mmcif/',\n",
    "                 starting_idx=0,\n",
    "                 limit=None,\n",
    "                 shuffle = False,\n",
    "                 reload=True,\n",
    "                 remove_hetatm=True,\n",
    "                 allow_exception=False):\n",
    "        self.path = path\n",
    "        self.structure_path = self.path + structure\n",
    "        self.path_table = path + 'gpcrdb/' + 'structures.pkl'\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.limit = limit\n",
    "        self.reload = reload\n",
    "        self.remove_hetatm = remove_hetatm\n",
    "        self.allow_exception = allow_exception\n",
    "        \n",
    "        self.filenames, self.pdb_ids = self.get_pdb_files()\n",
    "        self.filenames = self.filenames[starting_idx:]\n",
    "        self.pdb_ids = self.pdb_ids[starting_idx:]\n",
    "        if self.limit == None:\n",
    "            self.limit = len(self.pdb_ids)\n",
    "        if len(self.filenames) > self.limit:\n",
    "            self.filenames = self.filenames[:self.limit]\n",
    "            self.pdb_ids = self.pdb_ids[:self.limit]\n",
    "        # Columns for structure dataframe\n",
    "        self.cols = ['group_PDB', 'auth_asym_id', 'label_asym_id', 'label_seq_id', 'auth_seq_id', \n",
    "                     'label_comp_id', 'id', 'label_atom_id', \n",
    "                     'type_symbol', 'Cartn_x', 'Cartn_y', 'Cartn_z']\n",
    "        self.numbering = pd.DataFrame()\n",
    "                \n",
    "    # ==============================================================================================================\n",
    "    \n",
    "    def get_pdb_files(self):\n",
    "        # just a helper function that returns all pdb files in specified path\n",
    "        (_, _, filenames) = next(os.walk(self.structure_path))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(filenames)\n",
    "        files = [self.structure_path + x for x in filenames]\n",
    "        pdb_ids = list(set([x[-8:-4] for x in files]))\n",
    "        return files, pdb_ids\n",
    "    \n",
    "    def make_metainfo(self):\n",
    "        self.table = pd.read_pickle(self.path_table)\n",
    "        for i, pdb_id in tqdm(enumerate(self.pdb_ids)):\n",
    "            if i < self.limit:\n",
    "                protein, family = self.get_prot_info(pdb_id)\n",
    "                if protein == None:\n",
    "                    pass\n",
    "                else:\n",
    "                    numbering = self.get_res_nums(protein)\n",
    "                    if i == 0:\n",
    "                        self.mappings = self.get_mapping(pdb_id)\n",
    "                        numb = pd.DataFrame([pdb_id, protein, family, numbering]).T\n",
    "                        # numb = [pdb_id, protein, self.entry_to_ac(protein), family, numbering]\n",
    "                        numb.columns = ['PDB', 'identifier', 'family', 'numbering']\n",
    "                        self.numbering = self.numbering.append(numb)\n",
    "                    else:\n",
    "                        self.mappings = self.mappings.append(self.get_mapping(pdb_id), ignore_index=True)\n",
    "                        numb = pd.DataFrame(data=[pdb_id, protein, family, numbering]).T\n",
    "                        numb.columns = ['PDB', 'identifier', 'family', 'numbering']\n",
    "                        self.numbering = self.numbering.append(numb, ignore_index=True)\n",
    "\n",
    "    def make_raws(self):\n",
    "        for i, pdb_id in tqdm(enumerate(self.pdb_ids)):\n",
    "            if i < self.limit:\n",
    "                # only process if the file has not already been generated\n",
    "                # if not self.reload & \n",
    "                protein, family = self.get_prot_info(pdb_id)\n",
    "                if protein != None:\n",
    "                    if i == 0:\n",
    "                        self.structure = self.load_cifs(pdb_id)\n",
    "                        self.structure['identifier'] = protein.upper()\n",
    "                        if self.remove_hetatm:\n",
    "                            self.structure = self.structure[self.structure['group_PDB']!='HETATM']\n",
    "                            self.structure['label_seq_id'] = self.structure['label_seq_id'].astype(np.int64)\n",
    "                        self.structure['label_comp_sid'] = self.structure.apply(lambda x:\n",
    "                                                            gemmi.find_tabulated_residue(x.label_comp_id).one_letter_code, \n",
    "                                                            axis=1)\n",
    "                    else:\n",
    "                        structure = self.load_cifs(pdb_id)\n",
    "                        structure['identifier'] = protein.upper()\n",
    "                        if self.remove_hetatm:\n",
    "                            structure = structure[structure['group_PDB']!='HETATM']\n",
    "                            structure['label_seq_id'] = structure['label_seq_id'].astype(np.int64)\n",
    "                        structure['label_comp_sid'] = structure.apply(lambda x:\n",
    "                                                            gemmi.find_tabulated_residue(x.label_comp_id).one_letter_code, \n",
    "                                                            axis=1)\n",
    "                        self.structure = self.structure.append(structure, ignore_index=True)\n",
    "         \n",
    "    # ==============================================================================================================\n",
    "        \n",
    "    def entry_to_ac(self, entry: str):\n",
    "        query = 'https://www.uniprot.org/uniprot/'+entry+'.txt'\n",
    "        response = requests.get(query)\n",
    "        return response.text.split('\\n')[1].split('AC   ')[1][:6]\n",
    "    \n",
    "    def get_prot_info(self, pdb_id):\n",
    "        # query structure\n",
    "        query = 'https://gpcrdb.org/services/structure/'+pdb_id.upper()+'/'\n",
    "        response = requests.get(query)\n",
    "        if len(response.json()) > 0:\n",
    "            protein = response.json()['protein']\n",
    "            family = response.json()['family']\n",
    "            return protein, family\n",
    "        else:\n",
    "            return None, None\n",
    "    \n",
    "    def get_res_nums(self, protein):\n",
    "        # query uniprot -> res num\n",
    "        query = 'https://gpcrdb.org/services/residues/extended/'+protein+'/'\n",
    "        response = requests.get(query)\n",
    "        # select res num\n",
    "        # assign res_num to structure data\n",
    "        return response.json()\n",
    "    \n",
    "    def get_mapping(self, pdb_id):\n",
    "        maps = get_mappings_data(pdb_id)[pdb_id.lower()]['UniProt']\n",
    "        uniprots = maps.keys()\n",
    "        full_table=pd.DataFrame()\n",
    "        for i, uniprot in enumerate(uniprots):\n",
    "            table = pd.DataFrame.from_dict(maps[uniprot])\n",
    "            table['PDB'] = pdb_id\n",
    "            table['uniprot'] = uniprot\n",
    "            if i == 0:\n",
    "                full_table = table\n",
    "            else:\n",
    "                full_table = full_table.append(table, ignore_index=True)\n",
    "        return full_table\n",
    "    \n",
    "    # ==============================================================================================================\n",
    "    \n",
    "    def load_cifs(self, pdb_id):\n",
    "        path = 'data/mmcif/' + pdb_id + '.cif'\n",
    "        try:\n",
    "            doc = cif.read_file(path)  # copy all the data from mmCIF file\n",
    "            lol = []  # list of lists\n",
    "            for b, block in enumerate(doc):\n",
    "                table = block.find('_atom_site.', self.cols)\n",
    "                for row in table:\n",
    "                    lol.append([pdb_id]+list(row))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"Hoppla. %s\" % e)\n",
    "            sys.exit(1)\n",
    "        cols = ['PDB']+self.cols\n",
    "        st = gemmi.read_structure(path)\n",
    "        model = st[0]\n",
    "        if len(st) > 1:\n",
    "            print(\"There are multiple models!\")\n",
    "        rol = []\n",
    "        for chain in model:\n",
    "            for r, res in enumerate(chain.get_polymer()):\n",
    "                # previous_residue() and next_residue() return previous/next\n",
    "                # residue only if the residues are bonded. Otherwise -- None.\n",
    "                prev_res = chain.previous_residue(res)\n",
    "                next_res = chain.next_residue(res)\n",
    "                try:\n",
    "                    phi, psi = gemmi.calculate_phi_psi(prev_res, res, next_res)\n",
    "                except:\n",
    "                    phi, psi = np.nan, np.nan\n",
    "                try:\n",
    "                    omega = gemmi.calculate_omega(res, next_res)\n",
    "                except:\n",
    "                    omega = np.nan\n",
    "                rol.append([res.label_seq, res.subchain, \n",
    "                            degrees(phi), degrees(omega), degrees(psi)])\n",
    "        cols2 = ['label_seq_id', 'label_asym_id', 'phi', 'omega', 'psi']\n",
    "        rol_df = pd.DataFrame(data=rol, columns=cols2)\n",
    "        rol_df['label_seq_id'] = rol_df['label_seq_id'].astype(int)\n",
    "        lol_df = pd.DataFrame(data=lol, columns=cols)\n",
    "        lol_df['label_seq_id'] = lol_df.apply(lambda x: int(x.label_seq_id) if x.label_seq_id != '.' else np.nan, axis=1)\n",
    "        return pd.merge(lol_df, rol_df, how='inner', on=['label_asym_id', 'label_seq_id'])\n",
    "    \n",
    "    # ==============================================================================================================   \n",
    "            \n",
    "    def to_pkl_metainfo(self):\n",
    "        self.numbering.to_pickle(self.path + 'data_numbering.pkl')\n",
    "        self.table.to_pickle(self.path + 'data_table.pkl')\n",
    "        self.mappings.to_pickle(self.path + 'data_mappings.pkl')\n",
    "    \n",
    "    def to_pkl_raw(self, folder='data/raw/', overwrite=False):\n",
    "        for pdb_id in self.pdb_ids:\n",
    "            structure = self.structure[self.structure['PDB']==pdb_id]\n",
    "            if len(structure) >= 1:\n",
    "                if (not os.path.isfile(folder + pdb_id + '.pkl')) or overwrite:\n",
    "                    structure.to_pickle(folder + pdb_id + '.pkl')\n",
    "                    print(\"writing to file:\", folder + pdb_id + '.pkl')\n",
    "    \n",
    "    def to_pkl_processed(self, folder='data/processed/', overwrite=False):\n",
    "        for df in self.dfl:\n",
    "            pdb_id = df['PDB'].unique()[0]\n",
    "            if (not os.path.isfile(folder + pdb_id + '.pkl')) or overwrite:\n",
    "                df.to_pickle(folder + pdb_id + '.pkl')\n",
    "                print(\"writing to file:\", folder + pdb_id + '.pkl')\n",
    "    \n",
    "    # ==============================================================================================================   \n",
    "    \n",
    "    def del_pkl(self, folder='data/raw/'):\n",
    "        files = [f for f in os.listdir(folder) if '.pkl' in f]\n",
    "        for file in files:\n",
    "            os.remove(folder + file)\n",
    "    \n",
    "    def del_pkl_metainfo(self):\n",
    "        os.remove(self.path + 'data_numbering.pkl')\n",
    "        os.remove(self.path + 'data_table.pkl')\n",
    "        os.remove(self.path + 'data_mappings.pkl')\n",
    "            \n",
    "    # ==============================================================================================================\n",
    "    \n",
    "    def read_pkl_raw(self):\n",
    "        # not needed atm\n",
    "        pass\n",
    "    \n",
    "    def read_pkl_processed(self, folder='data/processed/'):\n",
    "        files = [f for f in os.listdir(folder) if '.pkl' in f]\n",
    "        self.dfl = []\n",
    "        for f in files:\n",
    "            self.dfl.append(pd.read_pickle(folder+f).reset_index().drop('index', axis=1))\n",
    "    \n",
    "    def read_pkl_metainfo(self):\n",
    "        self.numbering = pd.read_pickle(self.path + 'data_numbering.pkl')\n",
    "        self.table = pd.read_pickle(self.path + 'data_table.pkl')\n",
    "        self.mappings = pd.read_pickle(self.path + 'data_mappings.pkl')\n",
    "    \n",
    "    # ==============================================================================================================    \n",
    "    \n",
    "    def get_stacked_maps(self, pdb):\n",
    "        # add gene to mapping\n",
    "        mappings_ = self.mappings[self.mappings['PDB']==pdb]\n",
    "        pref_chain = self.table[self.table['PDB']==pdb.upper()]['Preferred Chain'].iloc[0]\n",
    "        map_df_list = []\n",
    "        for j in range(len(mappings_)):\n",
    "            chain = pd.DataFrame.from_dict(mappings_.iloc[j]['mappings'])['chain_id'].iloc[0]\n",
    "            identifier = mappings_.iloc[j]['name']\n",
    "            dict_ = pd.DataFrame.from_dict(mappings_.iloc[j]['mappings'])\n",
    "            dict_['identifier'] = identifier\n",
    "            map_df_list.append(pd.DataFrame.from_dict(dict_))\n",
    "        _ = pd.concat(map_df_list)\n",
    "        _ = _[_['chain_id']==pref_chain]\n",
    "        _['PDB'] = pdb\n",
    "        return _\n",
    "\n",
    "\n",
    "    def get_generic_nums(self, pdb_id):\n",
    "        sequence_numbers = []\n",
    "        amino_acids = []\n",
    "        generic_numbers = []\n",
    "        for i in self.numbering[self.numbering['PDB']==pdb_id].iloc[0]['numbering']:\n",
    "            if i['alternative_generic_numbers'] != []:\n",
    "                sequence_numbers.append(i['sequence_number'])\n",
    "                amino_acids.append(i['amino_acid'])\n",
    "                generic_numbers.append(i['display_generic_number'])\n",
    "        return list(zip(sequence_numbers, amino_acids, generic_numbers))\n",
    "\n",
    "\n",
    "    def get_generic_number(self, zipped_pos_dict, l2u, comp_sid):\n",
    "        if l2u >= 0:\n",
    "            if l2u in list(zip(*zipped_pos_dict))[0]:\n",
    "                idx = list(zip(*zipped_pos_dict))[0].index(l2u)\n",
    "                row = zipped_pos_dict[idx]\n",
    "                if row[1] == comp_sid:\n",
    "                    # print(\"found row\", row[1], float(row[2].split('x')[0]), int(row[2].split('x')[1]), comp_sid)\n",
    "                    return row[2], row[1], float(row[2].split('x')[0]), int(row[2].split('x')[1])\n",
    "                else:\n",
    "                    # print(\"found row, but residue are not the same\", row, comp_sid)\n",
    "                    return row[2]+'?', row[1], float(row[2].split('x')[0]), int(row[2].split('x')[1])\n",
    "            else:\n",
    "                return ['', '', 0, 0]\n",
    "        else:\n",
    "            return ['', '', 0, 0]\n",
    "\n",
    "\n",
    "    def assign_generic_numbers_(self, pdb_id, overwrite, folder):\n",
    "        data = pd.read_pickle(folder + pdb_id + '.pkl').reset_index().drop('index', axis=1)\n",
    "        print(\"loaded data to assign gen. numbers from\", folder + pdb_id + '.pkl')\n",
    "        cols = data.columns\n",
    "        columns = ['gen_pos', 'gen_pos1', 'gen_pos2', 'uniprot_comp_sid']\n",
    "        _ = [i for i in columns if i in cols]\n",
    "        if len(_) > 0:\n",
    "            if overwrite:\n",
    "                data.drop(_, axis=1, inplace=True)\n",
    "                data['label_2_uni'] = 0\n",
    "                data[columns[0]] = ''\n",
    "                data[columns[1]] = 0\n",
    "                data[columns[2]] = 0\n",
    "                data[columns[3]] = ''\n",
    "            else:\n",
    "                return data\n",
    "        else:\n",
    "            data['label_2_uni'] = 0\n",
    "            data[columns[0]] = ''\n",
    "            data[columns[1]] = 0\n",
    "            data[columns[2]] = 0\n",
    "            data[columns[3]] = ''\n",
    "        maps_stacked = self.get_stacked_maps(pdb_id)\n",
    "        if 'residue_number' in maps_stacked.index:\n",
    "            pass\n",
    "        else:\n",
    "            return data\n",
    "        if type(maps_stacked[maps_stacked['PDB']==pdb_id].\\\n",
    "                loc['residue_number'][['chain_id', 'start','end','unp_start','unp_end', 'identifier', 'PDB']])\\\n",
    "                    == pandas.core.series.Series:\n",
    "            pref_mapping = maps_stacked[maps_stacked['PDB']==pdb_id].loc['residue_number']\\\n",
    "                [['chain_id', 'start','end','unp_start','unp_end', 'identifier', 'PDB']].to_frame().T\n",
    "        else:\n",
    "            pref_mapping = maps_stacked[maps_stacked['PDB']==pdb_id].\\\n",
    "                loc['residue_number'][['chain_id', 'start','end','unp_start','unp_end', 'identifier', 'PDB']]\n",
    "        pref_chain = pref_mapping['chain_id'].iloc[0]\n",
    "        pref_mapping = pref_mapping.sort_values('start')\n",
    "        uniprot_identifier_ = data[data['PDB']==pdb_id]['identifier'].unique()\n",
    "        uniprot_identifier = uniprot_identifier_[0]\n",
    "        natoms = len(data[data['PDB']==pdb_id])\n",
    "        \n",
    "        for j in range(len(pref_mapping)):\n",
    "            row = pref_mapping.iloc[j].to_dict()\n",
    "            map_identifier = row['identifier']\n",
    "            map_pdb = row['PDB']\n",
    "            start_label_seq_id = row['start']\n",
    "            start_uniprot = row['unp_start']\n",
    "            end_label_seq_id = row['end']\n",
    "            end_uniprot = row['unp_end']\n",
    "            if map_identifier == uniprot_identifier:\n",
    "                idxs = [x for x in range(natoms+1) \\\n",
    "                        if ((x <= end_label_seq_id) & (x >= start_label_seq_id))]\n",
    "                vals = [x + start_uniprot - start_label_seq_id for x in range(natoms+1) \\\n",
    "                        if ((x <= end_label_seq_id) & (x >= start_label_seq_id))]\n",
    "                for k, idx in enumerate(idxs):\n",
    "                    line = data[(data['PDB'] == pdb_id) &\n",
    "                                (data['label_seq_id'] == idx) &\n",
    "                                (data['label_atom_id'] == 'CA')]\n",
    "                    lines = len(line)\n",
    "                    if len(line) > 1:\n",
    "                        line = line[line['auth_asym_id'] == pref_chain]\n",
    "                    if len(line) > 0:\n",
    "                        data.at[line.index[0], 'label_2_uni'] = int(vals[k])\n",
    "            else:\n",
    "                #print('Didnt find correct uniprotmap (not a gpcr):', map_identifier)\n",
    "                pass\n",
    "        # Generate generic numbers\n",
    "        zipped_pos_dict = self.get_generic_nums(pdb_id)\n",
    "        if type(data) == pandas.core.series.Series:\n",
    "            data = data.to_frame().T\n",
    "        \n",
    "        data[['gen_pos', 'uniprot_comp_sid', 'gen_pos1', 'gen_pos2']] = data.\\\n",
    "            apply(lambda x: self.get_generic_number(zipped_pos_dict, x.label_2_uni, x.label_comp_sid) if x.PDB==pdb_id\\\n",
    "                  else [x.gen_pos, x.uniprot_comp_sid, x.gen_pos1, x.gen_pos2], axis=1, result_type='expand')\n",
    "        return data\n",
    "    \n",
    "    def assign_generic_numbers(self, pdb_ids=None, overwrite=True, folder='data/raw/'):\n",
    "        dfl_ = []\n",
    "        if pdb_ids != None:\n",
    "            self.pdb_ids = pdb_ids\n",
    "        if not isinstance(self.pdb_ids, list):\n",
    "            self.pdb_ids = [self.pdb_ids]\n",
    "        for pdb_id in self.pdb_ids:\n",
    "            if self.allow_exception:\n",
    "                print(\"trying to assign generic nubmers to\", pdb_id)\n",
    "                try:\n",
    "                    dfl_.append(self.assign_generic_numbers_(pdb_id, overwrite=overwrite, folder=folder))\n",
    "                    print(\"assigned generic numbers to\", pdb_id, \"\\n\\n\\n\")\n",
    "                except:\n",
    "                    print(\"assigning failed for\", pdb_id)\n",
    "            else:\n",
    "                print(\"trying to assign generic nubmers to\", pdb_id)\n",
    "                dfl_.append(self.assign_generic_numbers_(pdb_id, overwrite=overwrite, folder=folder))\n",
    "                print(\"assigned generic numbers to\", pdb_id, \"\\n\\n\\n\")\n",
    "        self.dfl = dfl_\n",
    "        del dfl_\n",
    "        \n",
    "    # ==============================================================================================================\n",
    "    \n",
    "    def plot_angles_for_gen_pos(self, region=[7.40, 7.60], legend=True):\n",
    "        categories = []\n",
    "        for df in self.dfl:\n",
    "            category = np.unique(df['PDB'])[0]\n",
    "            categories.append(category)\n",
    "        # categories = list(set(categories))\n",
    "\n",
    "        cmap = plt.cm.get_cmap('RdYlGn', len(categories))\n",
    "        colors = np.linspace(0, len(categories)-1, len(categories))\n",
    "        colordict = dict(zip(categories,colors))\n",
    "\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=1,ncols=2,figsize=(24,12))\n",
    "\n",
    "        xlabel = 'generic position numbers'\n",
    "        ylabel = 'degrees'\n",
    "\n",
    "        xtick_min = region[0]\n",
    "        xtick_max = region[1]\n",
    "\n",
    "        nsteps = int(round(xtick_max - xtick_min, 2) / 0.01)\n",
    "        xticks = np.linspace(xtick_min, xtick_max, nsteps+1)\n",
    "\n",
    "        for i, cat in enumerate(categories):\n",
    "            c = int(colordict[cat])\n",
    "            df = self.dfl[i]\n",
    "            roi = df[(df['gen_pos1']>=region[0]) & \n",
    "                     (df['gen_pos1']<region[1]) & \n",
    "                     (df['label_atom_id']=='CA')].copy()\n",
    "            roi['phi'] = roi.apply(lambda x: float(x.phi), axis=1)\n",
    "            roi['omega'] = roi.apply(lambda x: float(x.omega), axis=1)\n",
    "            roi['psi'] = roi.apply(lambda x: float(x.psi), axis=1)\n",
    "            tot_len = len(roi['phi'].to_list())+len(roi['omega'].to_list())+len(roi['psi'].to_list())\n",
    "            if tot_len > 0:\n",
    "                roi.plot(kind='line', x='gen_pos1', y='phi', \n",
    "                        ax = axes[0], subplots = True, color = cmap(c), \n",
    "                        xticks = xticks, label=cat)\n",
    "                if legend:\n",
    "                    axes[0].legend(bbox_to_anchor=(1.0, 1.0))\n",
    "                axes[0].set_xlabel(xlabel)\n",
    "                axes[0].set_ylabel(ylabel)\n",
    "                axes[0].set_title('Psi Angles')\n",
    "\n",
    "                roi.plot(kind='line', x='gen_pos1', y='psi', \n",
    "                        ax = axes[1], subplots = True, color = cmap(c), \n",
    "                        xticks = xticks, label=cat)\n",
    "                if legend:\n",
    "                    axes[1].legend(bbox_to_anchor=(1.0, 1.0))\n",
    "                axes[1].set_xlabel(xlabel)\n",
    "                axes[1].set_ylabel(ylabel)\n",
    "                axes[1].set_title('Phi Angles')\n",
    "        plt.show()\n",
    "        \n",
    "    def get_family():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daily-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CifProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-agent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "federal-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbd include "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-jamaica",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "partial-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.read_pkl_processed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "breathing-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_pkl_metainfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "medium-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.make_metainfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "decimal-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.read_pkl_metainfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "simple-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_family(fam_id):\n",
    "    return fam_id.split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "major-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fam_id_to_df(fam_id, path='data/families.pkl'):\n",
    "    split = split_family(fam_id)\n",
    "    print(split)\n",
    "    family_df = get_families(path=path)\n",
    "    zipped = list(zip(['v1', 'v2', 'v3', 'v4'], split))\n",
    "    zipped = [(x[0], int(x[1])) for x in zipped if x[1] != '']\n",
    "    for z in zipped:\n",
    "        family = family[family[z[0]]==z[1]]\n",
    "    return family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "touched-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_family(numbering, family):\n",
    "    \"\"\"\n",
    "    family: a length = 4 list with 4 identifiers\n",
    "    \"\"\"\n",
    "    numbering[['f1', 'f2', 'f3', 'f4']] = numbering.apply(lambda x: x.family.split('_'), axis = 1, result_type='expand')\n",
    "    for i, f in enumerate(family):\n",
    "        if f != '':\n",
    "            col = 'f'+ str(i+1)\n",
    "            numbering = numbering[numbering[col] == family[i]]\n",
    "    return list(numbering['PDB'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "delayed-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_from_dfl(dfl, pdb_ids):\n",
    "    selection = []\n",
    "    sel_pdbs = []\n",
    "    for df in dfl:\n",
    "        if df['PDB'].unique()[0] in pdb_ids:\n",
    "            selection.append(df)\n",
    "            sel_pdbs.append(df['PDB'].unique()[0])\n",
    "    return selection, sel_pdbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "central-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity(table, sel_pdbs):\n",
    "    activity_dict = {}\n",
    "    actives = []\n",
    "    inactives = []\n",
    "    for pdb in sel_pdbs:\n",
    "        act = table[table['PDB']==pdb.upper()]['State'].iloc[0]\n",
    "        activity_dict.update({pdb: act})\n",
    "        if act == 'Active':\n",
    "            actives.append(pdb)\n",
    "        elif act == 'Inactive':\n",
    "            inactives.append(pdb)\n",
    "    return activity_dict, actives, inactives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "running-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_act_inact(data, family_id):\n",
    "    split = split_family(family_id)\n",
    "    group = group_by_family(data.numbering, split)\n",
    "    _, sel_pdbs = get_group_from_dfl(data.dfl, group)\n",
    "    _, actives, inactives = get_activity(data.table, sel_pdbs)\n",
    "    active_dfl = get_group_from_dfl(data.dfl, actives)[0]\n",
    "    inactive_dfl = get_group_from_dfl(data.dfl, inactives)[0]\n",
    "    family_name = family_id\n",
    "    return active_dfl, inactive_dfl, family_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "nutritional-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_families(path = 'data/families.txt'):\n",
    "    with open(path) as f:\n",
    "        cols = ['f1', 'v1', 'f2', 'v2', 'f3', 'v3', 'f4', 'v4']\n",
    "        family_df = pd.DataFrame(columns=cols)\n",
    "        v0_ = 0\n",
    "        v1_ = 0\n",
    "        v2_ = 0\n",
    "        v3_ = 0\n",
    "        for row in f.readlines():\n",
    "            print(row)\n",
    "            if not '    ' in row:\n",
    "                v0 = row.split('|')[0][:-1]\n",
    "                v0_ += 1\n",
    "                v1_ = 0\n",
    "                v2_ = 0\n",
    "                v3_ = 0\n",
    "            elif not '        ' in row:\n",
    "                v1 = row.replace(\"\\n\", \"\")[4:]\n",
    "                v1_ += 1\n",
    "                v2_ = 0\n",
    "                v3_ = 0\n",
    "            elif not '            ' in row:\n",
    "                v2 = row.replace(\"        \", \"\").replace(\"\\n\", \"\")\n",
    "                v2_ += 1\n",
    "                v3_ = 0\n",
    "            else:\n",
    "                try:\n",
    "                    v3 = row.split(',')[8]\n",
    "                    v3_ += 1\n",
    "                except:\n",
    "                    v3 = row.split(',')\n",
    "                    v3_ += 1\n",
    "                dict_ = dict(zip(cols, [v0, v0_, v1, v1_, v2, v2_, v3, v3_]))\n",
    "                family_df=family_df.append(dict_, ignore_index=True)\n",
    "    return family_df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "difficult-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_id = '001_001_003_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "olympic-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# family = fam_id_to_df(family_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "unable-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "objective-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_dfl, inactive_dfl, family_name = compare_act_inact(data, family_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fuzzy-radio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(active_dfl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "charitable-enclosure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inactive_dfl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-rubber",
   "metadata": {},
   "source": [
    "# PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "extreme-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_angles_for_gen_pos(dfl, region=[7.40, 7.60], legend=True, title='', save=False):\n",
    "    categories = []\n",
    "    for df in dfl:\n",
    "        category = np.unique(df['PDB'])[0]\n",
    "        categories.append(category.upper())\n",
    "\n",
    "    cmap = plt.cm.get_cmap('RdYlGn', len(categories))\n",
    "    colors = np.linspace(0, len(categories)-1, len(categories))\n",
    "    colordict = dict(zip(categories,colors))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(24,12))\n",
    "\n",
    "    xlabel = 'generic position numbers'\n",
    "    ylabel = 'degrees'\n",
    "    \n",
    "    start_regions = [x for _, x in enumerate(region) if _ % 2 == 0]\n",
    "    end_regions = [x for _, x in enumerate(region) if _ % 2 == 1]\n",
    "    \n",
    "    for i in range(len(start_regions)):\n",
    "        xtick_min = start_regions[i]\n",
    "        xtick_max = end_regions[i]\n",
    "        nsteps = int(round(xtick_max - xtick_min, 2) / 0.01)\n",
    "        if i == 0:\n",
    "            xticks = np.linspace(start_regions[i], end_regions[i], nsteps+1)\n",
    "        else:\n",
    "            xticks = np.hstack([xticks, np.linspace(start_regions[i], end_regions[i], nsteps+1)])\n",
    "    xticks = [round(x, 2) for x in xticks]\n",
    "    xticks_ = [i for i in range(len(xticks))]\n",
    "    xtick_dict = dict(zip(xticks, xticks_))\n",
    "    \n",
    "    for i, cat in enumerate(categories):\n",
    "        c = int(colordict[cat])\n",
    "        df = dfl[i]\n",
    "        roi = pd.concat([df.loc[(df['gen_pos1'] > start_regions[i]) &\n",
    "                                (df['gen_pos1'] < end_regions[i]) &\n",
    "                                (df['label_atom_id']=='CA')] for i in range(len(start_regions))], join='outer').copy()\n",
    "        try:\n",
    "            # better: check if this is not an empty series!\n",
    "            roi['xticks'] = roi.apply(lambda x: xtick_dict[x.gen_pos1], axis=1)\n",
    "        except:\n",
    "            roi['xticks'] = np.nan\n",
    "        roi['phi'] = roi.apply(lambda x: float(x.phi), axis=1)\n",
    "        roi['omega'] = roi.apply(lambda x: float(x.omega), axis=1)\n",
    "        roi['psi'] = roi.apply(lambda x: float(x.psi), axis=1)\n",
    "        tot_len = len(roi['phi'].to_list())+len(roi['omega'].to_list())+len(roi['psi'].to_list())\n",
    "        if tot_len > 0:\n",
    "            roi.plot(kind='line', x='xticks', y='phi', \n",
    "                    ax = axes[0], subplots = True, color = cmap(c), label=cat)\n",
    "            if legend:\n",
    "                axes[0].legend(bbox_to_anchor=(1.0, 1.0))\n",
    "            else:\n",
    "                axes[0].get_legend().remove()\n",
    "            axes[0].set_xticks(xticks_)\n",
    "            axes[0].set_xticklabels([str(\"%.2f\" % round(x,2)) for x in xticks], minor=False)\n",
    "            axes[0].set_ylim(-150, 50)\n",
    "            axes[0].set_xlabel(xlabel)\n",
    "            axes[0].set_ylabel(ylabel)\n",
    "            axes[0].set_title('Phi Angles')\n",
    "\n",
    "            roi.plot(kind='line', x='xticks', y='psi', \n",
    "                    ax = axes[1], subplots = True, color = cmap(c), label=cat)\n",
    "            if legend:\n",
    "                axes[1].legend(bbox_to_anchor=(1.0, 1.0))\n",
    "            else:\n",
    "                axes[1].get_legend().remove()\n",
    "            axes[1].set_xticks(xticks_)\n",
    "            axes[1].set_xticklabels([str(\"%.2f\" % round(x,2)) for x in xticks], minor=False)\n",
    "            axes[1].set_ylim(-150, 50)\n",
    "            axes[1].set_xlabel(xlabel)\n",
    "            axes[1].set_ylabel(ylabel)\n",
    "            axes[1].set_title('Psi Angles')\n",
    "    fig.suptitle(title, fontsize=12)\n",
    "    mplcursors.cursor(hover=True)\n",
    "    plt.show()\n",
    "    if save:\n",
    "        print(\"saving\")\n",
    "        plt.savefig('data/plots/'+title.replace(' ', '_')+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-crawford",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "accessible-boards",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "vietnamese-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_angles_for_gen_pos(active_dfl, region=[7.45, 7.56, 8.51, 8.54], legend=False, title='Active GPCRs (Complexes) '+family_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "increasing-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_angles_for_gen_pos(inactive_dfl, region=[7.45, 7.58, 8.51, 8.54], legend=False, title='Inactive GPCRs '+family_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-track",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-essex",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-valentine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "beneficial-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download gproteins structures?  ==> where do i get regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "internal-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict contact maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "scenic-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict activity based on contact maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-rabbit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-cookie",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "incident-prospect",
   "metadata": {},
   "source": [
    "# StructLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "improved-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8  load structs (structs dataloader) and table\n",
    "# btw how do i get the affinities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "medical-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9  plot structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "electronic-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 plot deltas (make interaction map based on genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "interior-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-reviewer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-lender",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-devon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "latter-container",
   "metadata": {},
   "source": [
    "# GPROTEINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "third-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/gproteins'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "gproteins_table = 'data/gproteins.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-crazy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-branch",
   "metadata": {},
   "source": [
    "# AFFINITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "affinities_table = 'data/affinities.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-particle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-afternoon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-beads",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
