{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c9a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pickle as pickle\n",
    "import plotly.graph_objects as go\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "from analysis.graphanalysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a86ab41",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec65e1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███                                                                                                               | 15/557 [00:00<00:03, 144.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files with generic numbers on receptors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 557/557 [00:03<00:00, 141.32it/s]\n"
     ]
    }
   ],
   "source": [
    "p = CifProcessor()\n",
    "p.read_pkl_metainfo()\n",
    "p.read_pkl(mode='r', folder='data/processed/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda3ef09",
   "metadata": {},
   "source": [
    "# Create graph analysis class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ee1ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from data/couplings/families_coupling.xls!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sideadmin\\.conda\\envs\\mt\\lib\\site-packages\\pandas\\core\\generic.py:4152: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Affinity Processor!\n",
      "Please set a group --------------  ['GPCRdb', 'Inoue', 'Bouvier'].\n",
      "please set label type -----------  ['Guide to Pharmacology', 'Log(Emax/EC50)', 'pEC50', 'Emax'].\n",
      "\n",
      "Selected label type 'Log(Emax/EC50)'.\n",
      "\n",
      "\n",
      "Selected data of group 'GPCRdb'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gp = GraphProcessor(d=[], p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c5bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.set_atom_list()\n",
    "gp.apply_atom_list_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b392c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 368/368 [00:02<00:00, 131.25it/s]\n"
     ]
    }
   ],
   "source": [
    "gp.simplify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4424eda9",
   "metadata": {},
   "source": [
    "# Create training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7163be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pois = ['1.55', '2.39', '3.46', '6.37', '7.55']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea154aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress, product\n",
    "\n",
    "def combinations(items):\n",
    "    return ( set(compress(items,mask)) for mask in product(*[[0,1]]*len(items)) )\n",
    "    # alternative:                      ...in product([0,1], repeat=len(items)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d51d3133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[set(),\n",
       " {3},\n",
       " {2},\n",
       " {2, 3},\n",
       " {1},\n",
       " {1, 3},\n",
       " {1, 2},\n",
       " {1, 2, 3},\n",
       " {0},\n",
       " {0, 3},\n",
       " {0, 2},\n",
       " {0, 2, 3},\n",
       " {0, 1},\n",
       " {0, 1, 3},\n",
       " {0, 1, 2},\n",
       " {0, 1, 2, 3}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(combinations(range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98e09e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'learning_rate': [0.00001],\n",
    "    'cons_r_res':[\n",
    "        ['3.53', '7.55'],\n",
    "        ['3.53', '6.37'],\n",
    "        ['6.37', '7.55'],\n",
    "        ['2.55', '3.53', '7.53']\n",
    "        \n",
    "    ],\n",
    "    'radius': [10],\n",
    "    'max_edge_dist': [7, 9],\n",
    "    'batch_size': [2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c68d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*hyper_params.items())\n",
    "keys = tuple(['index']) + keys\n",
    "configs = [dict(zip(keys, [idx, *v])) for idx, v in enumerate(itertools.product(*values))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ed832f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 0,\n",
       " 'learning_rate': 1e-05,\n",
       " 'cons_r_res': ['3.53', '7.55'],\n",
       " 'radius': 10,\n",
       " 'max_edge_dist': 7,\n",
       " 'batch_size': 2}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995370a6",
   "metadata": {},
   "source": [
    "# Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d611473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, n_epochs, lr, train_loader, validation_loader, patience = 50):\n",
    "    n_outputs=4\n",
    "    h5n = H5Net(n_outputs=n_outputs, aggr='add')\n",
    "    h5n.to('cuda')\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(h5n.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=patience, \n",
    "                                                           threshold=0.0001, threshold_mode='rel', \n",
    "                                                           cooldown=0, min_lr=0, eps=1e-8, verbose=True)\n",
    "\n",
    "    print(\"Initialized model!\")\n",
    "    n_epochs = n_epochs\n",
    "\n",
    "    t_e_losses = []\n",
    "    t_mse_losses = []\n",
    "    v_e_losses = []\n",
    "    v_mse_losses = []\n",
    "\n",
    "    best_validation = 12\n",
    "    best_training = 12\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        # TRAINING\n",
    "        training_loss = 0\n",
    "        t_bar = tqdm(enumerate(train_loader), desc='TRAINING')\n",
    "        for b,  batch in t_bar:\n",
    "            batch.z = batch.x\n",
    "            batch.to('cuda')\n",
    "            pred = h5n(batch)\n",
    "            target = batch.y / 12  # very crude normalization! (should substract upper & lower limit etc)\n",
    "            target = target.reshape(-1, n_outputs)\n",
    "            loss = criterion(pred, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss = (training_loss * b + loss) / (b + 1)\n",
    "            if b % 5 == 0:\n",
    "                t_bar.set_description(model + ' | E: {} | TRAINING: MSE = '.format(e)\n",
    "                                      +str(round(float(training_loss), 6)))\n",
    "        \n",
    "        t_mse_loss = training_loss.cpu().detach().numpy()\n",
    "        t_e_loss = round(torch.sqrt(training_loss).cpu().detach().numpy() * 12, 2)\n",
    "        t_e_losses.append(t_e_loss) # take the square root to get the mean error, then *12\n",
    "        t_mse_losses.append(t_mse_loss)\n",
    "        \n",
    "        if t_e_loss < best_training:\n",
    "            best_training = t_e_loss            \n",
    "        \n",
    "        # VALIDATION\n",
    "        with torch.no_grad():\n",
    "            validation_loss = 0\n",
    "            t_bar = tqdm(enumerate(validation_loader), desc='VALIDATION')\n",
    "            for b,  batch in t_bar:\n",
    "                batch.z = batch.x\n",
    "                batch.to('cuda')\n",
    "                pred = h5n(batch)\n",
    "                target = batch.y / 12\n",
    "                target = target.reshape(-1, n_outputs)\n",
    "                loss = criterion(pred, target)\n",
    "                validation_loss = (validation_loss * b + loss) / (b + 1)\n",
    "                if b % 5 == 0:\n",
    "                    t_bar.set_description(model + ' | E: {} | VALIDATION: MSE = '.format(e)\n",
    "                                          +str(round(float(validation_loss), 6)))\n",
    "        scheduler.step(validation_loss)\n",
    "        v_mse_loss = validation_loss.cpu().detach().numpy()\n",
    "        v_e_loss = round(torch.sqrt(validation_loss).cpu().detach().numpy() * 12, 2)\n",
    "        v_e_losses.append(v_e_loss)\n",
    "        v_mse_losses.append(v_mse_loss)\n",
    "        \n",
    "        if v_e_loss < best_validation:\n",
    "            print(\"New best validation perfomance: MSE={} | mean epoch error={}\".format(v_mse_loss, v_e_loss))\n",
    "            best_validation = v_e_loss\n",
    "            ckpt = h5n.state_dict()\n",
    "            last_update = e\n",
    "        \n",
    "        if e - (4 * patience) > last_update:\n",
    "            print(\"STOP (no further improvement recorded after {} epochs)\".format(4 * patience))\n",
    "            break       \n",
    "            \n",
    "    return ckpt, t_e_losses, t_mse_losses, best_training, v_e_losses, v_mse_losses, best_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f60a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "def run(model, gp, config, performance, n_epochs=250, save=True):\n",
    "    gp.create_graph(filter_by_chain=True,\n",
    "                gpcr=True,\n",
    "                gprotein=False,\n",
    "                auxilary=False,\n",
    "                node_criteria='Interaction Site', \n",
    "                edge_criteria='radius',\n",
    "                h5start=13,\n",
    "                cons_r_res=config['cons_r_res'], \n",
    "                radius=config['radius'],\n",
    "                max_edge_dist=config['max_edge_dist'])\n",
    "    print(\"Finished creating Graphs!\")\n",
    "    validation_split = .2\n",
    "    shuffle_dataset = True\n",
    "\n",
    "    # Creating data indices for training and validation splits\n",
    "    dataset_size = len(gp)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "    if shuffle_dataset :\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    # Creating PT data samplers and loaders\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "    \n",
    "    print(\"Initializing DataLoader ==> Batchsize: {}\".format(config['batch_size']))\n",
    "    train_loader = DataLoader(gp, batch_size=int(config['batch_size']), sampler=train_sampler)\n",
    "    validation_loader = DataLoader(gp, batch_size=int(config['batch_size']), sampler=valid_sampler)\n",
    "    \n",
    "    print(\"Starting Training ({} Epochs)!\".format(n_epochs))\n",
    "    ckpt, t_e_losses, t_mse_losses, best_training, v_e_losses, v_mse_losses, best_validation = run_training(\n",
    "        model=model,\n",
    "        n_epochs=n_epochs, \n",
    "        lr = config['learning_rate'],\n",
    "        train_loader=train_loader, \n",
    "        validation_loader=validation_loader)\n",
    "    performance[config['index']] = {\n",
    "        'best_training': best_training,\n",
    "        'training_epoch_losses': t_e_losses,\n",
    "        'training_mse_losses': t_mse_losses,\n",
    "        'best_validation': best_validation,\n",
    "        'validation_epoch_losses': v_e_losses,\n",
    "        'validation_mse_losses': v_mse_losses\n",
    "    }\n",
    "    if save:\n",
    "        picklefile = open(str('models/performances'+model+'.pkl'),'wb')\n",
    "        pickle.dump(performance, picklefile)\n",
    "    return performance, ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caed7320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'performance = {}\\n\\nfor c, config in enumerate(configs):\\n    if c in top5:\\n        values = list(config.values())\\n        model = \\'model_\\' + \"_\".join([str(x) if not isinstance(x, list) else \"_\".                                     join([str(round(float(y), 2)).replace(\".\", \"\") for y in x]) for x in values])\\n        np.random.seed(seed=c)\\n        torch.manual_seed(c)\\n        print(\"\\n\\n\\nRUNNING NEW CONFIGURATION:\\n\",config)\\n        performance, ckpt = run(model, gp, config, performance, n_epochs=1000)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"performance = {}\n",
    "\n",
    "for c, config in enumerate(configs):\n",
    "    if c in top5:\n",
    "        values = list(config.values())\n",
    "        model = 'model_' + \"_\".join([str(x) if not isinstance(x, list) else \"_\".\\\n",
    "                                     join([str(round(float(y), 2)).replace(\".\", \"\") for y in x]) for x in values])\n",
    "        np.random.seed(seed=c)\n",
    "        torch.manual_seed(c)\n",
    "        print(\"\\n\\n\\nRUNNING NEW CONFIGURATION:\\n\",config)\n",
    "        performance, ckpt = run(model, gp, config, performance, n_epochs=1000)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2590bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# picklefile = open(str('models/performances.pkl'),'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e94fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(performance, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f7b8539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"models/performances.pkl\", \"rb\") as input_file:\n",
    "#     data = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a22d6",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84cd53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def loss_plot(performance, config, save=False):\n",
    "    values = list(config.values())\n",
    "    model = 'model_' + \"_\".join([str(x) if not isinstance(x, list) else \"_\".\\\n",
    "                                 join([str(round(float(y), 2)).replace(\".\", \"\") for y in x]) for x in values])\n",
    "    n_epochs = len(performance['validation_epoch_losses'])\n",
    "    def make_plot(fig, a, b, col=0, loss='MSE'):\n",
    "        # Create traces\n",
    "        fig.add_trace(go.Scatter(\n",
    "            y=a,\n",
    "            mode='lines',\n",
    "            name='Validation loss'))\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            y=b,\n",
    "            mode='lines',\n",
    "            name='Training loss'))\n",
    "\n",
    "        y_title = loss + ' [log(Emax/EC50)]'\n",
    "        fig.update_layout(\n",
    "            title=model,\n",
    "            xaxis_title='Epoch',\n",
    "            yaxis_title=y_title\n",
    "            )\n",
    "        \n",
    "        vysmoothed = gaussian_filter1d(a, sigma=2)\n",
    "        fig.add_trace(go.Scatter(y=vysmoothed))\n",
    "        \n",
    "        tysmoothed = gaussian_filter1d(b, sigma=2)\n",
    "        fig.add_trace(go.Scatter(y=tysmoothed))\n",
    "        return fig\n",
    "    fig = go.Figure()\n",
    "    fig = make_plot(fig, performance['validation_epoch_losses'], performance['training_epoch_losses'], loss='Mean Absolute Error (MAE) \\n')\n",
    "    fig.show()\n",
    "    if save:\n",
    "        fig.write_image(\"plots/\" + model + \"mae_.png\")\n",
    "    del(fig)\n",
    "    fig = go.Figure()\n",
    "    fig = make_plot(fig, performance['validation_mse_losses'], performance['training_mse_losses'], loss='Mean Squared Error (MSE)')\n",
    "    fig.show()\n",
    "    if save:\n",
    "        fig.write_image(\"plots/\" + model + \"_mse.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447aa2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516cdd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(data.keys()):\n",
    "    loss_plot(data[i], config=configs[i], save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b00ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_config(performance):\n",
    "    best_vals = []\n",
    "    idxs = []\n",
    "    for i, p in enumerate(performance.values()):\n",
    "        best_vals.append(round(float(p['best_validation'].cpu().numpy()), 3))\n",
    "        idxs.append(i)\n",
    "    return [(y, x) for (x, y) in list(sorted(zip(best_vals, idxs)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee516cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = list(dict(get_best_config(performance)[:5]).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b9b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f4a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "[configs[x] for x in top5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d11ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_single_residue_variation(model, sample):\n",
    "    aas = [x for x in range(20)]\n",
    "    with torch.no_grad():\n",
    "        sample.to('cuda')\n",
    "        sample.z = sample.x\n",
    "        residues = [int(x) for x in list(sample.x)]\n",
    "        columns = [[str(x)+'_gs', str(x)+'_gi/o', str(x)+'_gq/11', str(x)+'_g12/13'] for x in residues]\n",
    "        cols = [item for sublist in columns for item in sublist]\n",
    "        df = pd.DataFrame(columns=cols, index=aas)\n",
    "        affinities = list(model(sample)[0].cpu().numpy()*12)\n",
    "        print(residues)\n",
    "        for r, res in enumerate(residues):\n",
    "            for i, aa in enumerate(aas):\n",
    "                if res == aa:\n",
    "                    for a, aff in enumerate(affinities):\n",
    "                        df.at[aa, cols[r*4+a]] = aff\n",
    "                else:\n",
    "                    sample_ = sample\n",
    "                    sample_.z[r] = aa\n",
    "                    affinities_ = list(model(sample_)[0].cpu().numpy()*12)\n",
    "                    for a, aff in enumerate(affinities_):\n",
    "                        df.at[aa, cols[r*4+a]] = aff\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e654d389",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size  = 1\n",
    "\n",
    "# Creating data indices for training and validation splits\n",
    "dataset_size = len(gp)\n",
    "\n",
    "analyse_loader = DataLoader(gp, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8e6451",
   "metadata": {},
   "source": [
    "# in vitro Mutation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(validation_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ecc833",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analyse_single_residue_variation(h5n, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aa5689",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cols = [x for x in list(df.columns) if 's' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e3d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T.std(axis=1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47017d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2af2a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7140e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
